{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch_scatter\n",
    "import torch_geometric\n",
    "import torch_sparse\n",
    "import ogb\n",
    "from tensorboard import SummaryWriter\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the data set\n",
    "from ogb.graphproppred import PygGraphPropPredDataset, Evaluator\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch_geometric.transforms as T\n",
    "from tqdm import tqdm\n",
    "\n",
    "dataset_name = 'ogbg-molhiv'\n",
    "dataset = PygGraphPropPredDataset(dataset_name, transform=None)\n",
    "device = 'cpu'\n",
    "print('Device: {}'.format(device))\n",
    "\n",
    "split_idx = dataset.get_idx_split()\n",
    "\n",
    "print('Dataset task type: {}'.format(dataset.task_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset[split_idx['train']], batch_size=32, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(dataset[split_idx['test']], batch_size=32, shuffle=True, num_workers=0)\n",
    "valid_loader = DataLoader(dataset[split_idx['valid']], batch_size=32, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_parameters = {\n",
    "    'device': device,\n",
    "    'num_layers': 3,\n",
    "    'hidden_dim': 1024,\n",
    "    'dropout': 0.5,\n",
    "    'lr': 0.001,\n",
    "    'epochs': 10,\n",
    "}\n",
    "hyper_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim, dropout, return_embeds=False):\n",
    "        super(GCN, self).__init__()\n",
    "        \n",
    "        self.convs = torch.nn.ModuleList(\n",
    "            [GCNConv(input_dim, hidden_dim)] \n",
    "            +[GCNConv(hidden_dim, hidden_dim) for i in range(num_layers - 2)] \n",
    "            +[GCNConv(hidden_dim, output_dim)]\n",
    "            )\n",
    "        \n",
    "        self.batchnorm = torch.nn.ModuleList([torch.nn.BatchNorm1d(hidden_dim) for i in range(num_layers - 1)])\n",
    "\n",
    "        self.softmax = torch.nn.LogSoftmax()\n",
    "        self.dropout = dropout\n",
    "        self.return_embeds = return_embeds\n",
    "\n",
    "        \n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for conv_layer in self.convs:\n",
    "            conv_layer.reset_parameters()\n",
    "        for batchnorm_layer in self.batchnorm:\n",
    "            batchnorm_layer.reset_parameters()\n",
    "\n",
    "    def forward(self, x, adj_t):\n",
    "        \n",
    "        for i in range(len(self.batchnorm)):\n",
    "            x = self.convs[i](x, adj_t)\n",
    "            x = self.batchnorm[i](x)\n",
    "\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training, inplace=False) # self.training was inherited from nn.Module assign is training or evaluation\n",
    "            \n",
    "        x = self.convs[-1](x, adj_t)\n",
    "\n",
    "        if not self.return_embeds:\n",
    "            x = self.softmax(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ogb.graphproppred.mol_encoder import AtomEncoder\n",
    "from torch_geometric.nn import global_add_pool, global_mean_pool\n",
    "\n",
    "class GCN_Graph(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_dim, output_dim, num_layers, dropout):\n",
    "        super(GCN_Graph, self).__init__()\n",
    "\n",
    "        self.node_encoder = AtomEncoder(hidden_dim) ## Encoders for atoms in molecule graphs\n",
    "        self.gnn_node = GCN(hidden_dim, hidden_dim, num_layers, hidden_dim, dropout, return_embeds=True)\n",
    "        self.pool = global_mean_pool\n",
    "        self.linear = torch.nn.Linear(hidden_dim, output_dim) ## Output layer\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        self.gnn_node.reset_parameters()\n",
    "        self.linear.reset_parameters()\n",
    "\n",
    "    def forward(self, batched_data):\n",
    "        x = batched_data.x\n",
    "        edge_index = batched_data.edge_index\n",
    "        batch = batched_data.batch\n",
    "\n",
    "        embedings = self.node_encoder(x) ## node_encoder generates emeddings with size hidden_dims\n",
    "        embedings = self.gnn_node(embedings, edge_index) ## gnn_node takes hidden_dim dimensional embeddings as input and outputs hidden_dim dimensional convolutionized embeddings\n",
    "        embedings = self.pool(embedings, batch)\n",
    "        output = self.linear(embedings)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, data_loader, optimizer, loss_func):\n",
    "    model.train()\n",
    "    loss=0\n",
    "    \n",
    "    for step, batch in enumerate(tqdm(data_loader, desc=\"Iteration\")):\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        if batch.x.shape[0] == 1 or batch.batch[-1] == 0:\n",
    "            pass\n",
    "        else:\n",
    "            is_labeled = batch.y == batch.y\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch)\n",
    "            \n",
    "        \n",
    "            loss = loss_func(output[is_labeled], batch.y[is_labeled].type(torch.float32))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "    return loss.item()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, device, loader, evaluator, save_model_results=False, save_file=None):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    for step, batch in enumerate(tqdm(loader, desc=\"Iteration\")):\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        if batch.x.shape[0] == 1:\n",
    "            pass\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                pred = model(batch)\n",
    "\n",
    "            y_true.append(batch.y.view(pred.shape).detach().cpu())\n",
    "            y_pred.append(pred.detach().cpu())\n",
    "\n",
    "    y_true = torch.cat(y_true, dim = 0).numpy()\n",
    "    y_pred = torch.cat(y_pred, dim = 0).numpy()\n",
    "\n",
    "    input_dict = {\"y_true\": y_true, \"y_pred\": y_pred}\n",
    "\n",
    "    if save_model_results:\n",
    "        print (\"Saving Model Predictions\")\n",
    "        \n",
    "        # Create a pandas dataframe with a two columns\n",
    "        # y_pred | y_true\n",
    "        data = {}\n",
    "        data['y_pred'] = y_pred.reshape(-1)\n",
    "        data['y_true'] = y_true.reshape(-1)\n",
    "\n",
    "        df = pd.DataFrame(data=data)\n",
    "        # Save to csv\n",
    "        df.to_csv('ogbg-molhiv_graph_' + save_file + '.csv', sep=',', index=False)\n",
    "\n",
    "    return evaluator.eval(input_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GCN_Graph(hyper_parameters['hidden_dim'],\n",
    "                  dataset.num_tasks, hyper_parameters['num_layers'],\n",
    "                  hyper_parameters['dropout']).to(device)\n",
    "evaluator = Evaluator(name='ogbg-molhiv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "model.reset_parameters()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=hyper_parameters['lr'])\n",
    "loss_func = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "best_model = None\n",
    "best_valid_acc = 0\n",
    "\n",
    "\n",
    "for epoch in range(1, 1 + hyper_parameters[\"epochs\"]):\n",
    "  print('Training...')\n",
    "  loss = train(model, device, train_loader, optimizer, loss_func)\n",
    "\n",
    "  print('Evaluating...')\n",
    "  train_result = eval(model, device, train_loader, evaluator)\n",
    "  val_result = eval(model, device, valid_loader, evaluator)\n",
    "  test_result = eval(model, device, test_loader, evaluator)\n",
    "\n",
    "  train_acc, valid_acc, test_acc = train_result[dataset.eval_metric], val_result[dataset.eval_metric], test_result[dataset.eval_metric]\n",
    "  if valid_acc > best_valid_acc:\n",
    "      best_valid_acc = valid_acc\n",
    "      best_model = copy.deepcopy(model)\n",
    "\n",
    "  print(f'Epoch: {epoch:02d}, '\n",
    "        f'Loss: {loss:.4f}, '\n",
    "        f'Train: {100 * train_acc:.2f}%, '\n",
    "        f'Valid: {100 * valid_acc:.2f}% '\n",
    "        f'Test: {100 * test_acc:.2f}%')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnnenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "218cbdc887a37ae401e17b5146eb2ec71151ac5014f17cc50bd48c4b3427e13e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
